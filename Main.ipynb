{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "import string\n",
    "from random import choice\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "stopwords_.update(['amp','https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label(data, col): -> object, object\n",
    "    '''\n",
    "    Divides text into topics using pca, Doc2Vec, and kmeans clustering.\n",
    "    Then creates graphs of each category.\n",
    "        Args:\n",
    "            Dataframe data: dataframe to model into topics\n",
    "            string col: column that contains text to vectorize\n",
    "        Returns:\n",
    "            original dataframe,\n",
    "            matplotlib subplots of graphs\n",
    "    '''\n",
    "    \n",
    "    tweets = data[col]\n",
    "    tagged_X = [TaggedDocument(t,[i]) for i, t in enumerate(tweets)]\n",
    "\n",
    "    d2v_model = Doc2Vec(tagged_X, vector_size = 140, epochs = 30,\n",
    "                    min_count = 500, workers=10, dm = 1, alpha=0.025)\n",
    "\n",
    "    d2v_model.train(tagged_X, total_examples=d2v_model.corpus_count, \n",
    "                    epochs=30, start_alpha=0.002, end_alpha=-0.016)\n",
    "    kmeans_model = KMeans(n_clusters=3, init='k-means++', max_iter=100)\n",
    "    vectors_docs = d2v_model.docvecs.vectors_docs\n",
    "    X = kmeans_model.fit(vectors_docs)\n",
    "    labels=kmeans_model.labels_.tolist()\n",
    "    l = kmeans_model.fit_predict(vectors_docs)\n",
    "    pca = PCA(n_components=2).fit(vectors_docs)\n",
    "    datapoint = pca.transform(vectors_docs)\n",
    "    fig, ax = plt.subplots()\n",
    "    labelc = ['#fc5603', '#03fcf8', '#3103fc', '#0b170d']\n",
    "    colors = [labelc[i] for i in labels]\n",
    "    ax.scatter(datapoint[:, 0], datapoint[:, 1], c=colors, s =1)\n",
    "    centroids = kmeans_model.cluster_centers_\n",
    "\n",
    "    centroidpoint = pca.transform(centroids)\n",
    "    ax.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=100, c='#ffff00')\n",
    "    plt.show()\n",
    "    \n",
    "    data['cluster_label'] = labels\n",
    "    return data,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_TFIDF(corpus): -> object\n",
    "     '''\n",
    "     Stemmatizes corpus, then returns its tfidf matrix.\n",
    "        Args:\n",
    "            Series corpus: a series of strings\n",
    "        Returns:\n",
    "            Object Tfidf matrix of corpus\n",
    "    '''\n",
    "    # stem words\n",
    "    snowb = SnowballStemmer('english')\n",
    "    corpus = ['\\n'.join([snowb.stem(word) for word in word_tokenize(tweet)])\n",
    "                    for tweet in corpus.values]\n",
    "    # vectorize w/ ngrams\n",
    "    vec = TfidfVectorizer(ngram_range = (1,3), tokenizer = word_tokenize, \n",
    "                          stop_words = set(wordcloud.STOPWORDS))\n",
    "    return vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(preds, y): -> float, float, float, object\n",
    "     '''\n",
    "     Tests predictions to actuals and baseline.\n",
    "        Args:\n",
    "            Series preds: predicted labels\n",
    "            Series y: actual labels\n",
    "        Returns:\n",
    "            float: test accuracy score\n",
    "            float: baseline accuracy score\n",
    "            float: test f1 score\n",
    "            np.array: test confusion matrix\n",
    "    '''\n",
    "        \n",
    "    # make a baseline that's just uniform random guesses.\n",
    "    random_preds = [choice([0,1]) for _ in range(len(y))]\n",
    "    meanbase = len(preds[preds == random_preds])/len(preds)\n",
    "    # baselines that always guess 1 xor 0\n",
    "    bases = [(i, len(preds[preds == i])/len(preds)) for i in [0,1]]\n",
    "    bases = bases + [('random',meanbase)]\n",
    "    \n",
    "    #model test accuracy\n",
    "    acc = ('acc', len(preds[preds == y])/len(preds))\n",
    "    \n",
    "    # calculate f1 score\n",
    "    f1 = ('f1', f1_score(preds, y))\n",
    "    \n",
    "    conf_mat = confusion_matrix(y, preds)\n",
    "    \n",
    "    return acc, bases, f1, conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(data, text_col, stopwords):-> list\n",
    "    '''\n",
    "    Returns the top 20 words in a category by tfidf score\n",
    "        Args:\n",
    "            Dataframe data: dataframe of category\n",
    "            string text_col: name of colums that includes text\n",
    "            list stopwords: list of stopwords\n",
    "        Returns:\n",
    "            list - list of 20 most occurring words in category\n",
    "    '''\n",
    "    #lemmatize words\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = data[text_col].apply(lem.lemmatize)\n",
    "    vec = TfidfVectorizer(tokenizer = word_tokenize, stop_words=stopwords)\n",
    "    vec.fit(corpus)\n",
    "    words = pd.Series(vec.idf_, index = vec.get_feature_names())\n",
    "    top_words = words.sort_values()[:20]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_cluster_words(df, text, stopwords, filename): -> None\n",
    "    '''\n",
    "    Graphs top words of a topic by modified tfidf score.\n",
    "    Saves graph as png.\n",
    "        Args:\n",
    "            Dataframe df: dataframe of topic\n",
    "            string text: name of text column\n",
    "            list stopwords: list of stopwords\n",
    "            string filename: name to save file as\n",
    "        returns:\n",
    "            None\n",
    "    '''\n",
    "    fontA = {'family' : 'normal',\n",
    "        'size'   : 16}\n",
    "    fontB = {'family' : 'normal',\n",
    "            'rotation': 30,\n",
    "            'size'   : 14}\n",
    "    \n",
    "    colors = ['purple','green', 'orange']\n",
    "    for i, c in enumerate(colors):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        plt.xlabel('Word', **fontA)\n",
    "        plt.ylabel('Topic Importance',**fontA)\n",
    "        plt.title('Sub-Topic '+str(i+1), **fontA)\n",
    "        tw = top_words(df[df['cluster_label'] == i], text, stopwords)\n",
    "        tw = 11 - tw\n",
    "        x_pos = [i for i, _ in enumerate(tw)]\n",
    "        ax.bar(x_pos, tw.values, color = c)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(tw.index, **fontB)\n",
    "        fig.savefig(filename+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load & clean primary tweets for use\n",
    "\n",
    "# 174 days\n",
    "days_raw = [31,30,31,30,31,21] # total days of months between 3/1/2020 - 8/22/2020\n",
    "labels = ['pos', 'neg']\n",
    "dates = []\n",
    "\n",
    "# Stored every data/sentiment on a different csv \n",
    "# so I could scrape using cheap machines without straining their memory.\n",
    "primary_tweets = pd.DataFrame()\n",
    "for label in labels:\n",
    "    for month_num, month_days in enumerate(days_raw):\n",
    "        month = []\n",
    "        for i in range(month_days):\n",
    "            filepath = 'tweets_prim/'+label+'2020-{:02d}'.format(month_num+3)+'-{:02d}'.format(i+1)+'.csv'\n",
    "            if label == 'pos':\n",
    "                dates.append('2020-{:02d}'.format(month_num+3)+'-{:02d}'.format(i+1))\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                primary_tweets = primary_tweets.append(df)\n",
    "            except:\n",
    "                print(filepath, 'not found')\n",
    "\n",
    "# Get a date column in addition to datetime                \n",
    "primary_tweets['date'] = primary_tweets['datetime_tweet'].apply(lambda x: x[:x.find(' ')])\n",
    "# fully clean clean_text\n",
    "primary_tweets = primary_tweets.replace({'label': {'pos': 1, 'neg': 0}})\n",
    "primary_tweets['clean_text'] = primary_tweets['clean_text'].apply(lambda x: str(x).lower().replace('wwg1wga','wwgwga'))\n",
    "primary_tweets['clean_text'] = primary_tweets['clean_text'].apply(lambda x:''.join([str(i) for i in x \n",
    "                                                  if i.isalpha() or i.isspace()]))\n",
    "#split by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_pos = primary_tweets[primary_tweets['label'] == 1]\n",
    "primary_neg = primary_tweets[primary_tweets['label'] == 0]\n",
    "primary_pos,_ = cluster_label(primary_pos, 'clean_text')\n",
    "primary_neg,_ = cluster_label(primary_neg, 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds via secndary tweets\n",
    "# uses pure word count, not tfidf\n",
    "\n",
    "c_stopwords = set(wordcloud.STOPWORDS)\n",
    "c_stopwords.update([\"will\",'1','2','3','4','s','t', 've', 'll','re', \"now\", \"https\", \"amp\", \"mask\",'people',\n",
    "                    'wear','one','dont','masks','wearing', '’', 'covid',\"”\",\"“\", 'im', 'go', 'see',\n",
    "                    'covid19','coronavirus','don','igshid','want','make','face','say','need','us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = series_to_TFIDF(primary_tweets['clean_text'])\n",
    "y = primary_tweets['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & predict w/ Naive Bayes\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "test_results(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cluster_words(primary_neg, 'clean_text', c_stopwords, 'neg')\n",
    "for i in range(3):\n",
    "    samp = primary_neg[primary_neg['cluster_label'] == i].sample(10)\n",
    "    samp = samp[['username', 'raw_text']]\n",
    "    samp.to_csv(str(i)+'neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cluster_words(primary_pos, 'clean_text', c_stopwords, 'pos')\n",
    "for i in range(3):\n",
    "    samp = primary_pos[primary_pos['cluster_label'] == i].sample(10)\n",
    "    samp = samp[['username', 'raw_text']]\n",
    "    samp.to_csv(str(i)+'pos.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
